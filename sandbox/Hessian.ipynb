{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c899d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9779a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_product(xs, ys):\n",
    "    \"\"\"\n",
    "    the inner product of two lists of variables xs,ys\n",
    "    :param xs:\n",
    "    :param ys:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def hessian_vector_product(gradsH, params, v):\n",
    "    \"\"\"\n",
    "    compute the hessian vector product of Hv, where\n",
    "    gradsH is the gradient at the current point,\n",
    "    params is the corresponding variables,\n",
    "    v is the vector.\n",
    "    \"\"\"\n",
    "    hv = torch.autograd.grad(gradsH, params, grad_outputs = v, only_inputs = True, retain_graph = True)\n",
    "    return hv\n",
    "\n",
    "def normalization(v):\n",
    "    \"\"\"\n",
    "    normalization of a list of vectors\n",
    "    return: normalized vectors v\n",
    "    \"\"\"\n",
    "    s = group_product(v,v)\n",
    "#     print(\"Before\", v)\n",
    "    s = s ** 0.5\n",
    "    s = s.cpu().item()\n",
    "    v = [vi / (s + 1e-6) for vi in v]\n",
    "#     print(\"After\", v)\n",
    "#     print()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1e2aef17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.2301,  0.1524, -0.3231]),\n",
       " tensor([0.4411, 0.2285, 0.3329]),\n",
       " tensor([ 0.3264,  0.4851, -0.3463])]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.Tensor([[0.3895, 0.2580, -0.5468],\n",
    "                   [0.7466, 0.3868, 0.5635],\n",
    "                   [0.5524, 0.8211, -0.5861]])\n",
    "normalization(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e22bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward(create_graph = True)\n",
    "\n",
    "params, gradsH = get_params_grad(model)\n",
    "v = [torch.randn(p.size()).to(device) for p in params]\n",
    "v = normalization(v)\n",
    "\n",
    "eigenvalue = None\n",
    "\n",
    "for i in range(maxIter):\n",
    "    model.zero_grad()\n",
    "    Hv = hessian_vector_product(gradsH, params, v)\n",
    "    eigenvalue_tmp = group_product(Hv, v).cpu().item()\n",
    "    v = normalization(Hv)\n",
    "    if eigenvalue == None:\n",
    "        eigenvalue = eigenvalue_tmp\n",
    "    else:\n",
    "        if abs(eigenvalue-eigenvalue_tmp)/abs(eigenvalue) < tol:\n",
    "            return eigenvalue_tmp, v\n",
    "        else:\n",
    "            eigenvalue = eigenvalue_tmp\n",
    "return eigenvalue, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1382cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume weight already has the grad information\n",
    "def top_hessian_eigen(weights):\n",
    "    v = torch.randn(weights.size())\n",
    "    eigenvalue, eigenvector = None, None\n",
    "    \n",
    "    for i in range(1000):\n",
    "        # outputs will contain graph from inputs to outputs\n",
    "        # inputs act as a marker in the graph to say where to do backprop until\n",
    "        # grad_outputs is the gradient wrt to the outputs\n",
    "            # for example, say we want the gradient of loss wrt to inputsA\n",
    "            # but we already have gradient of loss wrt to inputsB\n",
    "            # then gradient of loss wrt inputsA = gradient of loss wrt inputsB * gradients of inputsB wrt inputsA\n",
    "            # assuming the only path from inputsA to loss passes through inputsB\n",
    "        Hv = torch.autograd.grad(weights.grad, # outputs\n",
    "                                 weights, # inputs\n",
    "                                 grad_outputs=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf7a6781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  4.],\n",
      "        [ 8., 12.]], grad_fn=<CopyBackwards>)\n",
      "here\n",
      "Inputs tensor([[0., 1.],\n",
      "        [2., 3.]], requires_grad=True)\n",
      "Grads tensor([[ 0.,  4.],\n",
      "        [ 8., 12.]], grad_fn=<CopyBackwards>)\n",
      "\n",
      "[tensor([[ 0.4404,  0.1241],\n",
      "        [ 0.5275, -0.7158]])]\n",
      "(tensor([[ 1.7616,  0.4965],\n",
      "        [ 2.1101, -2.8631]]),)\n",
      "\n",
      "[tensor([[ 0.4404,  0.1241],\n",
      "        [ 0.5275, -0.7158]])]\n",
      "(tensor([[ 1.7616,  0.4965],\n",
      "        [ 2.1101, -2.8631]]),)\n",
      "\n",
      "3.9999983310699463 [tensor([[ 0.4404,  0.1241],\n",
      "        [ 0.5275, -0.7158]])]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def reduce(x):\n",
    "    return (x*x*2).sum()\n",
    "\n",
    "inputs = torch.tensor([[0, 1],\n",
    "                       [2, 3.0]], requires_grad=True).float()\n",
    "outputs = reduce(inputs)\n",
    "outputs.backward(create_graph=True)\n",
    "\n",
    "# grad = torch.autograd.grad(outputs, inputs2, retain_graph=True, create_graph=True)\n",
    "# print(grad)\n",
    "print(inputs.grad)\n",
    "print(\"here\")\n",
    "\n",
    "v = [torch.randn(inputs.size())]\n",
    "v = normalization(v)\n",
    "\n",
    "inputs = (inputs)\n",
    "grads = (inputs.grad)\n",
    "print(\"Inputs\", inputs)\n",
    "print(\"Grads\", grads)\n",
    "print()\n",
    "\n",
    "eigenvalue = None\n",
    "\n",
    "tol = 0.000001\n",
    "\n",
    "for i in range(10000):\n",
    "#     (Hv,) = torch.autograd.grad(inputs.grad, \n",
    "#                                 inputs, \n",
    "#                                 grad_outputs=v,\n",
    "#                                 retain_graph=True)\n",
    "\n",
    "    print(v)\n",
    "    Hv = hessian_vector_product(grads, inputs, v)\n",
    "    eigenvalue_tmp = group_product(Hv, v).cpu().item()\n",
    "#     print(\"Hv\", Hv)\n",
    "    print(Hv)\n",
    "    print()\n",
    "    v = normalization(Hv)\n",
    "    if eigenvalue == None:\n",
    "        eigenvalue = eigenvalue_tmp\n",
    "    else:\n",
    "        if abs(eigenvalue-eigenvalue_tmp)/abs(eigenvalue) < tol:\n",
    "            eigenvalue = eigenvalue_tmp\n",
    "            break\n",
    "        else:\n",
    "            eigenvalue = eigenvalue_tmp\n",
    "\n",
    "print(eigenvalue, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c79fdf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.1500e+01,  1.1624e+04,  5.9096e+04,  1.9954e+05],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([-3.1500e+01,  1.1624e+04,  5.9096e+04,  1.9954e+05])\n",
      "tensor([[ 3906.5000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000, 11680.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000, 29594.5000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000, 66560.0000]])\n"
     ]
    }
   ],
   "source": [
    "J = torch.autograd.functional.jacobian(mse, inputs, create_graph=False, strict=False, vectorize=False)\n",
    "H = torch.autograd.functional.hessian(mse, inputs, create_graph=False, strict=False, vectorize=False).reshape(4, 4)\n",
    "print(inputs.grad)\n",
    "print(J)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "807a5dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3906.5+0.j, 11680. +0.j, 29594.5+0.j, 66560. +0.j],\n",
       "      dtype=complex64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.linalg as la\n",
    "results = la.eigvals(H.numpy())\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7de44c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10., grad_fn=<SumBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]], requires_grad=True)\n",
      "tensor([[0., 2.],\n",
      "        [0., 6.]])\n",
      "\n",
      "tensor(10., grad_fn=<SumBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]], requires_grad=True)\n",
      "tensor([[0., 2.],\n",
      "        [0., 6.]], grad_fn=<ViewBackward>)\n",
      "Parameter containing:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-c80de7817edf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-171-c80de7817edf>\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-c80de7817edf>\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(y, x, create_graph)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgrad_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mgrad_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mjac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mgrad_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import torch                    \n",
    "\n",
    "def jacobian(y, x, create_graph=False): \n",
    "    print(y)\n",
    "    print(x)\n",
    "    jac = []                                                                                          \n",
    "    flat_y = y.reshape(-1)                                                                            \n",
    "    grad_y = torch.zeros_like(flat_y)                                                                 \n",
    "    for i in range(len(flat_y)):                                                                      \n",
    "        grad_y[i] = 1.                                                                                \n",
    "        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)\n",
    "        jac.append(grad_x.reshape(x.shape))                                                           \n",
    "        grad_y[i] = 0.                                                                                \n",
    "    return torch.stack(jac).reshape(y.shape + x.shape)\n",
    "\n",
    "def hessian(y, x):                                                                                    \n",
    "    return jacobian(jacobian(y, x, create_graph=True), x)            \n",
    "\n",
    "print(jacobian(func(layer, y), layer.weight))   \n",
    "print()\n",
    "print(hessian(func(layer, y), layer.weight))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "108dc8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Parameter containing:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]], requires_grad=True)\n",
      "grad tensor([[  0.,  50.],\n",
      "        [  0., 150.]], grad_fn=<CopyBackwards>)\n",
      "v [tensor([[-0.1826, -0.3651],\n",
      "        [-0.5477, -0.7303]])]\n",
      "50.0 [tensor([[ 0.0000, -0.4472],\n",
      "        [ 0.0000, -0.8944]])]\n"
     ]
    }
   ],
   "source": [
    "def my_normalize(v):\n",
    "    factor = (v*v).sum() ** 0.5\n",
    "    return v / ((v*v).sum() ** 0.5)\n",
    "\n",
    "x = torch.tensor([[0, 1.0], [2, 3]], requires_grad=True)\n",
    "\n",
    "layer = torch.nn.Linear(2, 2, bias=False)\n",
    "layer.weight = torch.nn.parameter.Parameter(x)\n",
    "y = torch.tensor([0, 5.0], requires_grad=True)\n",
    "\n",
    "\n",
    "def func(layer, y):\n",
    "#     m = nn.ReLU()\n",
    "#     loss = nn.CrossEntropyLoss()\n",
    "#     target = torch.empty(2, dtype=torch.long).random_(2)\n",
    "    output = layer(y)\n",
    "#     return loss(output, target)\n",
    "#     return layer(y).sum()\n",
    "    return (output * output).sum()\n",
    "\n",
    "func(layer, y).backward(create_graph=True)\n",
    "\n",
    "print(\"Weight\", layer.weight)\n",
    "print(\"grad\", layer.weight.grad)\n",
    "\n",
    "v = [torch.Tensor([[-1, -2], [-3, -4.0]])]\n",
    "v = normalization(v)\n",
    "print(\"v\", v)\n",
    "\n",
    "for i in range(10000):\n",
    "    Hv = hessian_vector_product(layer.weight.grad, layer.weight, v)\n",
    "    eigenvalue_tmp = group_product(Hv, v).cpu().item()\n",
    "    v = normalization(Hv)\n",
    "    if eigenvalue == None:\n",
    "        eigenvalue = eigenvalue_tmp\n",
    "    else:\n",
    "        if abs(eigenvalue-eigenvalue_tmp)/abs(eigenvalue) < tol:\n",
    "            eigenvalue = eigenvalue_tmp\n",
    "            break\n",
    "        else:\n",
    "            eigenvalue = eigenvalue_tmp\n",
    "\n",
    "print(eigenvalue, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
